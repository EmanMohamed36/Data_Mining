# -*- coding: utf-8 -*-
"""Mining 2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cKi2x1LoC6ZDqjWr00k8qY8XAj5nD7-w
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix
from sklearn import metrics
import seaborn as sns
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
# Load the CSV file into a pandas dataframe


def load_csv(file_path):
    df = pd.read_csv(file_path)
    return df

# Determine features and goal
def determine_features_and_goal(df):
    features = df.drop(columns=['y'])
    goal = df['y']
    return features, goal

# Split data into training and testing
def split_data(features, goal, test_size=0.5, random_state=3):
    X_train, X_test, y_train, y_test = train_test_split(features, goal, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

# Label encode categorical features
def label_encode_categorical_features(df): 
    le = LabelEncoder() #transform to 0 and 1
    df = df.apply(le.fit_transform)
    return df

# Solve missing values
def solve_missing(df):
    imputer = SimpleImputer(strategy="most_frequent") #mode defult -> mean
    df = imputer.fit_transform(df)
    return df


# Apply KNN classifier
def apply_knn_classifier(K,X_train, X_test, y_train):
    knn = KNeighborsClassifier(n_neighbors=K)
    knn.fit(X_train, y_train) #tranining
    return knn.predict(X_test)

# Apply Naive Bayes classifier
def apply_naive_bayes_classifier(X_train, X_test, y_train):
    gnb = GaussianNB()
    gnb.fit(X_train, y_train)
    return gnb.predict(X_test)

# Apply decision tree classifier
def apply_decision_tree_classifier(X_train, X_test, y_train):
    dt = DecisionTreeClassifier()
    dt.fit(X_train, y_train)
    return dt.predict(X_test),dt

#Apply randomForest classifier
from sklearn.ensemble import RandomForestClassifier
def apply_random_forest_classifier(X_train, X_test, y_train):
 rm = RandomForestClassifier(n_estimators = 10, max_depth=25, criterion = "gini", min_samples_split=10)
 rm.fit(X_train, y_train)
 rm_prd = rm.predict(X_test)
 return rm,rm_prd


# Calculate performance using confusion matrix
def calculate_performance(y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    v=round(metrics.accuracy_score(y_test, y_pred)*100)
    w=round(metrics.precision_score(y_test, y_pred, average='macro')*100)
    z=round(metrics.recall_score(y_test, y_pred, average='macro')*100)
    return v,w,z,cm

######main##############


if __name__ == '__main__':
    file_path = 'testDM.csv'
    df = load_csv(file_path)
    df=df.replace([np.inf, -np.inf, 'unknown'], np.nan).dropna()

    x,y=df.shape

    features, goal = determine_features_and_goal(df)
 
    target=goal.tolist()

    target= list(set(goal)) # you can also use unique(target) to get the unique values of the list
  
    F=list(features)

   
    df=solve_missing(df)
    print(df)


    X_train, X_test, y_train, y_test = split_data(features, goal, test_size = 0.3, random_state = 3)
    X_train = label_encode_categorical_features(X_train)
    X_test = label_encode_categorical_features(X_test)
   
  
    # Step3 : Apply first classifier  "KNN"
    K=round(np.sqrt(x))
    y_pred_knn = apply_knn_classifier(K,X_train, X_test, y_train)

    # Step3.1 : Calculate the performance of first classifier  "KNN" and print results
    A_res,P_res,R_res,con=calculate_performance(y_test,y_pred_knn)
    print("KNN report\n",classification_report(y_test, y_pred_knn, target_names=target))
    print("\n KNN Accuracy:",A_res,'%')
    print("KNN Precision:",P_res,'%')
    print("KNN Recall:",R_res,'%')
    print("KNN Confusion Matrix:\n",con)

    # Step 4:  Apply second classifier  "Naive Bayes"
    y_pred_nb = apply_naive_bayes_classifier(X_train, X_test, y_train)

    # Step 4.1: Calculate the performance of second classifier  "Naive Bayes" and print results
    A_res,P_res,R_res,con=calculate_performance(y_test, y_pred_nb)
    print("Naive report\n",classification_report(y_test, y_pred_nb, target_names=target))
    print("\n NaiveBayesAccuracy: ",A_res,'%')
    print("NaiveBayes Precision:",P_res,'%')
    print("NaiveBayes Recall:",R_res,'%')
    print("Naive Confusion Matrix:\n",con)

    # Step 5: Apply the third classifier  "Decision tree"
    y_pred_dt,dt = apply_decision_tree_classifier(X_train, X_test, y_train)

    # Step 5.1: Calculate the performance of the third classifier  "Decision tree" and print results
    A_res,P_res,R_res,con=calculate_performance(y_test, y_pred_dt)
    print("DT report\n",classification_report(y_test, y_pred_dt, target_names=target))
    print("\n Decision Tree Accuracy: ",A_res,'%')
    print("Decision Tree Precision:",P_res,'%')
    print("Decision Tree Recall:",R_res,'%')
    print("DT Confusion Matrix:\n",con)

    # Use this code to draw the decision tree and save it in tree.png
    from sklearn import tree
    import graphviz
    dot_data = tree.export_graphviz(dt, feature_names=F, class_names=target, filled=True, rounded=True, special_characters=True,out_file=None,)
    graph = graphviz.Source(dot_data)
    graph.format = "png"
    graph.render("Tree")

# Create list of top most features based on importance using information gain from Decision tree
feature_names = X_train.columns
feature_imports = dt.feature_importances_
most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=["Feature", "Importance"]).nlargest(10, "Importance")
most_imp_features.sort_values(by="Importance", inplace=True)
plt.figure(figsize=(10,6))
plt.barh(range(len(most_imp_features)), most_imp_features.Importance, align='center', alpha=0.4,color="green")
plt.yticks(range(len(most_imp_features)), most_imp_features.Feature, fontsize=14)
plt.xlabel('Importance')
plt.title('Most important features - Decision Tree')
plt.show()
